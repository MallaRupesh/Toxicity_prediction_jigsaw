{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "# from werkzeug import filename\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import skew, boxcox\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "import streamlit as stl\n",
    "\n",
    "\n",
    "def final_func_1(x):\n",
    "    \n",
    "    '''Final function 1 handling all the cleaning and loading of tokenizer , vectorizer and model and predicting '''\n",
    "    #Using regex and string manipulation to clean the data \n",
    "\n",
    "\n",
    "    def clean(data):\n",
    "        #https://www.kaggle.com/andrej0marinchenko/jigsaw-ensemble-0-86/notebook\n",
    "        data = data.str.replace('https?://\\S+|www\\.\\S+', ' social medium ')      \n",
    "\n",
    "        data = data.str.lower()\n",
    "        data = data.str.replace(\"4\", \"a\") \n",
    "        data = data.str.replace(\"2\", \"l\")\n",
    "        data = data.str.replace(\"5\", \"s\") \n",
    "        data = data.str.replace(\"1\", \"i\") \n",
    "        data = data.str.replace(\"!\", \"i\") \n",
    "        data = data.str.replace(\"|\", \"i\") \n",
    "        data = data.str.replace(\"0\", \"o\") \n",
    "        data = data.str.replace(\"8\", \"ate\") \n",
    "        data = data.str.replace(\"3\", \"e\") \n",
    "        data = data.str.replace(\"9\", \"g\")\n",
    "        data = data.str.replace(\"6\", \"g\")\n",
    "        data = data.str.replace(\"@\", \"a\")\n",
    "        data = data.str.replace(\"$\", \"s\")\n",
    "        data = data.str.replace(\"l3\", \"b\") \n",
    "        data = data.str.replace(\"7\", \"t\") \n",
    "        data = data.str.replace(\"7\", \"+\") \n",
    "\n",
    "        data = data.str.replace(\"#ofc\", \" of fuckin course \")\n",
    "        data = data.str.replace(\"fggt\", \" faggot \")\n",
    "        data = data.str.replace(\"your\", \" your \")\n",
    "        data = data.str.replace(\"self\", \" self \")\n",
    "        data = data.str.replace(\"cuntbag\", \" cunt bag \")\n",
    "        data = data.str.replace(\"fartchina\", \" fart china \")    \n",
    "        data = data.str.replace(\"youi\", \" you i \")\n",
    "        data = data.str.replace(\"cunti\", \" cunt i \")\n",
    "        data = data.str.replace(\"sucki\", \" suck i \")\n",
    "        data = data.str.replace(\"pagedelete\", \" page delete \")\n",
    "        data = data.str.replace(\"cuntsi\", \" cuntsi \")\n",
    "        data = data.str.replace(\"i'm\", \" i am \")\n",
    "        data = data.str.replace(\"offuck\", \" of fuck \")\n",
    "        data = data.str.replace(\"centraliststupid\", \" central ist stupid \")\n",
    "        data = data.str.replace(\"hitleri\", \" hitler i \")\n",
    "        data = data.str.replace(\"i've\", \" i have \")\n",
    "        data = data.str.replace(\"i'll\", \" sick \")\n",
    "        data = data.str.replace(\"fuck\", \" fuck \")\n",
    "        data = data.str.replace(\"f u c k\", \" fuck \")\n",
    "        data = data.str.replace(\"shit\", \" shit \")\n",
    "        data = data.str.replace(\"bunksteve\", \" bunk steve \")\n",
    "        data = data.str.replace('wikipedia', ' social medium ')\n",
    "        data = data.str.replace(\"faggot\", \" faggot \")\n",
    "        data = data.str.replace(\"delanoy\", \" delanoy \")\n",
    "        data = data.str.replace(\"jewish\", \" jewish \")\n",
    "        data = data.str.replace(\"sexsex\", \" sex \")\n",
    "        data = data.str.replace(\"allii\", \" all ii \")\n",
    "        data = data.str.replace(\"i'd\", \" i had \")\n",
    "        data = data.str.replace(\"'s\", \" is \")\n",
    "        data = data.str.replace(\"youbollocks\", \" you bollocks \")\n",
    "        data = data.str.replace(\"dick\", \" dick \")\n",
    "        data = data.str.replace(\"cuntsi\", \" cuntsi \")\n",
    "        data = data.str.replace(\"mothjer\", \" mother \")\n",
    "        data = data.str.replace(\"cuntfranks\", \" cunt \")\n",
    "        data = data.str.replace(\"ullmann\", \" jewish \")\n",
    "        data = data.str.replace(\"mr.\", \" mister \")\n",
    "        data = data.str.replace(\"aidsaids\", \" aids \")\n",
    "        data = data.str.replace(\"njgw\", \" nigger \")\n",
    "        data = data.str.replace(\"wiki\", \" social medium \")\n",
    "        data = data.str.replace(\"administrator\", \" admin \")\n",
    "        data = data.str.replace(\"gamaliel\", \" jewish \")\n",
    "        data = data.str.replace(\"rvv\", \" vanadalism \")\n",
    "        data = data.str.replace(\"admins\", \" admin \")\n",
    "        data = data.str.replace(\"pensnsnniensnsn\", \" penis \")\n",
    "        data = data.str.replace(\"pneis\", \" penis \")\n",
    "        data = data.str.replace(\"pennnis\", \" penis \")\n",
    "        data = data.str.replace(\"pov.\", \" point of view \")\n",
    "        data = data.str.replace(\"vandalising\", \" vandalism \")\n",
    "        data = data.str.replace(\"cock\", \" dick \")\n",
    "        data = data.str.replace(\"asshole\", \" asshole \")\n",
    "        data = data.str.replace(\"youi\", \" you \")\n",
    "        data = data.str.replace(\"afd\", \" all fucking day \")\n",
    "        data = data.str.replace(\"sockpuppets\", \" sockpuppetry \")\n",
    "        data = data.str.replace(\"iiprick\", \" iprick \")\n",
    "        data = data.str.replace(\"penisi\", \" penis \")\n",
    "        data = data.str.replace(\"warrior\", \" warrior \")\n",
    "        data = data.str.replace(\"loil\", \" laughing out insanely loud \")\n",
    "        data = data.str.replace(\"vandalise\", \" vanadalism \")\n",
    "        data = data.str.replace(\"helli\", \" helli \")\n",
    "        data = data.str.replace(\"lunchablesi\", \" lunchablesi \")\n",
    "        data = data.str.replace(\"special\", \" special \")\n",
    "        data = data.str.replace(\"ilol\", \" i lol \")\n",
    "        data = data.str.replace(r'\\b[uU]\\b', 'you')\n",
    "        data = data.str.replace(r\"what's\", \"what is \")\n",
    "        data = data.str.replace(r\"\\'s\", \" is \")\n",
    "        data = data.str.replace(r\"\\'ve\", \" have \")\n",
    "        data = data.str.replace(r\"can't\", \"cannot \")\n",
    "        data = data.str.replace(r\"n't\", \" not \")\n",
    "        data = data.str.replace(r\"i'm\", \"i am \")\n",
    "        data = data.str.replace(r\"\\'re\", \" are \")\n",
    "        data = data.str.replace(r\"\\'d\", \" would \")\n",
    "        data = data.str.replace(r\"\\'ll\", \" will \")\n",
    "        data = data.str.replace(r\"\\'scuse\", \" excuse \")\n",
    "        data = data.str.replace('\\s+', ' ')  \n",
    "        data = data.str.replace(r'(.)\\1+', r'\\1\\1') \n",
    "        data = data.str.replace(\"[:|♣|'|§|♠|*|/|?|=|%|&|-|#|•|~|^|>|<|►|_]\", '')\n",
    "\n",
    "\n",
    "        data = data.str.replace(r\"what's\", \"what is \")    \n",
    "        data = data.str.replace(r\"\\'ve\", \" have \")\n",
    "        data = data.str.replace(r\"can't\", \"cannot \")\n",
    "        data = data.str.replace(r\"n't\", \" not \")\n",
    "        data = data.str.replace(r\"i'm\", \"i am \")\n",
    "        data = data.str.replace(r\"\\'re\", \" are \")\n",
    "        data = data.str.replace(r\"\\'d\", \" would \")\n",
    "        data = data.str.replace(r\"\\'ll\", \" will \")\n",
    "        data = data.str.replace(r\"\\'scuse\", \" excuse \")\n",
    "        data = data.str.replace(r\"\\'s\", \" \")\n",
    "\n",
    "        # Clean some punctutations\n",
    "        data = data.str.replace('\\n', ' \\n ')\n",
    "        data = data.str.replace(r'([a-zA-Z]+)([/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n",
    "        # Replace repeating characters more than 3 times to length of 3\n",
    "        data = data.str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')   \n",
    "        # Add space around repeating characters\n",
    "        data = data.str.replace(r'([*!?\\']+)',r' \\1 ')    \n",
    "        # patterns with repeating characters \n",
    "        data = data.str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n",
    "        data = data.str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n",
    "        data = data.str.replace(r'[ ]{2,}',' ').str.strip()   \n",
    "        data = data.str.replace(r'[ ]{2,}',' ').str.strip()   \n",
    "#         data = data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "        return data\n",
    "\n",
    "    \n",
    "    # Loading the pre trained bert tokenizer \n",
    "    with open('tokenizer_new.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    " \n",
    "    # Loading the pre trained tfidf vectorizer\n",
    "    # def dummy_fun(doc):\n",
    "    #     return doc\n",
    "\n",
    "\n",
    "\n",
    "    with open('vectorizer_new.pickle', 'rb') as handle:\n",
    "        \n",
    "        vectorizer = pickle.load(handle)\n",
    "\n",
    "    # Loading the model\n",
    "#     regressor = pickle.load(open(\"/Users/rupesh/Downloads/Toxicity /finalized_model.sav\", 'rb'))\n",
    "    with open('regressor_new.pickle', 'rb') as handle:\n",
    "        \n",
    "        regressor = pickle.load(handle)\n",
    "\n",
    "#     cleaning the text \n",
    "    x = clean(x)\n",
    "\n",
    "    tokenized_comments = tokenizer(x.tolist())['input_ids']\n",
    "\n",
    "    comments_tr = vectorizer.fit_transform(tokenized_comments)\n",
    "\n",
    "\n",
    "    preds = regressor.predict(comments_tr)\n",
    "\n",
    "    with open('pipeline_new.pickle', 'rb') as handle:\n",
    "        \n",
    "        pipeline= pickle.load(handle)\n",
    "\n",
    "    preds = pipeline.predict(x)  \n",
    "\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "st.title(\"Predicting Toxicity of comments\")\n",
    "st.header('This app is created to predict how toxic a comment is')\n",
    "\n",
    "text1 = st.text_area('Enter text')\n",
    "\n",
    "text1=pd.Series(text1)\n",
    "if(stl.button('Submit')):\n",
    "\toutput =final_func_1(text1)\n",
    "\n",
    "\n",
    "\tstl.text(\"The level of toxicity :\")\n",
    "\tmy_bar = st.progress(float(output))\n",
    "\tst.success(f\"The output score is : {output}\")\n",
    "\n",
    "\tst.balloons()\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
